import math
import random
from abc import ABC
from pprint import pprint
from typing import (
    List,
    Optional,
    Tuple,
    )

# matplotlib.use('tkagg')
import numpy as np

# sys.path.append('../examples/flatirons')
# import func_tools
from ..data_logging.data_recorder import DataRecorder
from .ask_tell_optimizer import AskTellOptimizer
# import shapely
from .dimension.dimension_info import DimensionInfo


class GAOptimizer(AskTellOptimizer, ABC):
    """
    A simple genetic algorithm optimizer
    """
    
    def __init__(self,
                 generation_size: int = 100,
                 selection_proportion: float = .33,
                 dimensions: Optional[List[DimensionInfo]] = None,
                 ) -> None:
        self._recorder: Optional[DataRecorder] = None
        self._dimensions: [DimensionInfo] = [] if dimensions is None else dimensions
        self._generation_size: int = generation_size
        self._selection_proportion: float = selection_proportion
        
        self._num_parents = 2
        self._prob_virtual_parent = 0.0
        self._mutation_rate = .25
        
        self._population: [Tuple[float, any]] = []
    
    def setup(self, dimensions: [DimensionInfo], recorder: DataRecorder) -> None:
        """
        Setup parameters given initial conditions of the candidate
        :param dimensions: list of search dimensions
        :param recorder: data recorder
        """
        self._dimensions = dimensions
        self._recorder = recorder
        self._recorder.add_columns('generation', 'population')
    
    def stop(self) -> bool:
        """
        :return: True when the optimizer thinks it has reached a stopping point
        """
        return False
    
    def ask(self, num: Optional[int] = None) -> [any]:
        """
        :param num: the number of search points to return. If undefined, the optimizer will choose how many to return.
        :return: a list of search points generated by the optimizer
        """
        num = self._generation_size if num is None else num
        
        candidates = []
        for _ in range(num):
            candidate = np.empty(self.get_num_dimensions())
            
            # TODO: make each step here modular
            
            # select parents
            parents: [Tuple[float, any]] = []
            for i in range(self._num_parents):
                parent = None
                if len(self._population) <= self._num_parents or random.random() < self._prob_virtual_parent:
                    parent = self.make_virtual_candidate()
                else:
                    parent = self._population[random.randrange(0, len(self._population))]
                
                parents.append(parent)
            
            # combine parents
            for i, dimension in enumerate(self._dimensions):
                source_idx = random.randrange(0, len(parents))
                candidate[i] = parents[source_idx][1][i]
            
            # mutate
            for i, dimension in enumerate(self._dimensions):
                candidate[i] = \
                    self._mutation_rate * dimension.sample() + \
                    (1.0 - self._mutation_rate) * candidate[i]
            
            candidates.append(candidate)
        
        return candidates
    
    def tell(self, evaluations: [Tuple[float, any]]) -> None:
        """
        Updates the optimizer with the objective evaluations of a list of search points
        :param evaluations: a list of tuples of (evaluation, search point)
        """
        
        # TODO: make population update modular
        
        self._population.extend(evaluations)
        self._population.sort(key=lambda evaluation: evaluation[0], reverse=True)
        
        selection_size = math.ceil(self._selection_proportion * len(evaluations))
        del self._population[selection_size:]
        
        for i, dimension in enumerate(self._dimensions):
            dimension.update([evaluation[1][i] for evaluation in self._population])
        
        self._recorder.accumulate(evaluations, self._population)
        
        pprint(self._population[0:10])
    
    def best_solution(self) -> (Optional[float], any):
        """
        :return: the current best solution
        """
        return (None, self.mu()) if len(self._population) <= 0 else self._population[0]
    
    def mu(self) -> any:
        """
        :return: the current best solution
        """
        return np.fromiter((dimension.best() for dimension in self._dimensions), float)
    
    def get_num_candidates(self) -> int:
        return self._generation_size
    
    def get_num_dimensions(self) -> int:
        return len(self._dimensions)
    
    def make_virtual_candidate(self):
        candidate = (None, np.empty(self.get_num_dimensions()))
        for i, dimension in enumerate(self._dimensions):
            candidate[1][i] = dimension.sample()
        return candidate
